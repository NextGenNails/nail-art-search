import os
import pickle
import numpy as np
from typing import List, Dict, Any, Optional
import requests
from PIL import Image
import io
import torch
from transformers import CLIPProcessor, CLIPModel

# Global variables to cache model and processor
_model = None
_processor = None

def get_clip_model():
    """Get or load your trained CLIP model and processor."""
    global _model, _processor
    
    if _model is None or _processor is None:
        print("Loading trained CLIP model...")
        
        # Option 1: Load from Hugging Face Hub (if you uploaded it)
        # model_name = "your-username/your-trained-clip-model"
        
        # Option 2: Load from local directory (if you saved it locally)
        model_path = "../models"  # Your trained model from Colab
        
        # Option 3: Load from a specific checkpoint
        # model_path = "path/to/your/checkpoint-1000"
        
        try:
            # Try to load your trained model
            _model = CLIPModel.from_pretrained(model_path)
            _processor = CLIPProcessor.from_pretrained(model_path)
            print(f"âœ… Loaded trained CLIP model from {model_path}")
        except Exception as e:
            print(f"âŒ Failed to load trained model: {e}")
            print("ðŸ”„ Falling back to original CLIP model...")
            
            # Fallback to original model
            model_name = "openai/clip-vit-large-patch14"
            _model = CLIPModel.from_pretrained(model_name)
            _processor = CLIPProcessor.from_pretrained(model_name)
        
        # Move to GPU if available
        device = "cuda" if torch.cuda.is_available() else "cpu"
        _model = _model.to(device)
        print(f"CLIP model loaded on {device}")
    
    return _model, _processor

def preprocess_image_consistently(image_bytes: bytes) -> bytes:
    """
    Preprocess image consistently for both index building and querying.
    This ensures exact same processing pipeline to get 99-100% similarity.
    
    Args:
        image_bytes: Raw image bytes
        
    Returns:
        Preprocessed image bytes
    """
    try:
        # Convert bytes to PIL Image
        image = Image.open(io.BytesIO(image_bytes))
        
        # Convert to RGB if needed
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Resize to CLIP standard size (224x224)
        image = image.resize((224, 224), Image.Resampling.LANCZOS)
        
        # Convert back to bytes with consistent quality
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG', quality=95, optimize=False)
        img_byte_arr = img_byte_arr.getvalue()
        
        return img_byte_arr
        
    except Exception as e:
        print(f"Error preprocessing image: {str(e)}")
        return image_bytes

def get_clip_embedding(image_bytes: bytes) -> np.ndarray:
    """
    Generate CLIP embedding for an image using your trained model.
    
    Args:
        image_bytes: Raw image bytes
        
    Returns:
        CLIP embedding as numpy array (768 dimensions)
    """
    try:
        # Preprocess image consistently
        processed_bytes = preprocess_image_consistently(image_bytes)
        
        # Load model and processor
        model, processor = get_clip_model()
        
        # Convert processed bytes to PIL Image
        image = Image.open(io.BytesIO(processed_bytes))
        
        # Process image with CLIP processor
        inputs = processor(images=image, return_tensors="pt")
        
        # Move inputs to same device as model
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate embedding
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
            
        # Convert to numpy and normalize
        embedding = image_features.cpu().numpy().astype(np.float32)
        
        # Normalize for cosine similarity
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding.flatten()
        
    except Exception as e:
        print(f"Error generating CLIP embedding: {str(e)}")
        
        # Fallback to mock embedding if CLIP fails
        import hashlib
        import random
        
        image_hash = hashlib.md5(image_bytes).hexdigest()
        random.seed(int(image_hash[:8], 16))
        
        # Generate a 768-dimensional embedding (same as CLIP-L/14)
        embedding = np.random.normal(0, 1, 768).astype(np.float32)
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding

# Rest of your existing functions remain the same...
def download_image(url: str) -> Optional[bytes]:
    """Download image from URL."""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"Failed to download image from {url}: {str(e)}")
        return None

def build_index(image_paths: List[str], metadata: List[Dict[str, Any]], 
                index_path: str = "nail_art_index.faiss", 
                metadata_path: str = "nail_art_metadata.pkl") -> None:
    """Build FAISS index from image paths and metadata."""
    import faiss
    
    embeddings = []
    valid_metadata = []
    
    print(f"Processing {len(image_paths)} images...")
    
    for i, (image_path, meta) in enumerate(zip(image_paths, metadata)):
        try:
            # Read image file
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            
            # Generate embedding using your trained model
            embedding = get_clip_embedding(image_bytes)
            embeddings.append(embedding)
            valid_metadata.append(meta)
            
            if (i + 1) % 10 == 0:
                print(f"Processed {i + 1}/{len(image_paths)} images")
                
        except Exception as e:
            print(f"Failed to process {image_path}: {str(e)}")
            continue
    
    if not embeddings:
        raise Exception("No valid embeddings generated")
    
    # Convert to numpy array
    embeddings_array = np.array(embeddings, dtype=np.float32)
    
    # Build FAISS index
    dimension = embeddings_array.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
    
    # Add vectors to index
    index.add(embeddings_array)
    
    # Save index and metadata
    faiss.write_index(index, index_path)
    
    with open(metadata_path, 'wb') as f:
        pickle.dump(valid_metadata, f)
    
    print(f"Built index with {len(embeddings)} vectors")
    print(f"Index saved to {index_path}")
    print(f"Metadata saved to {metadata_path}")

def build_index_from_urls(image_urls: List[str], metadata: List[Dict[str, Any]],
                         download_dir: str = "downloaded_images",
                         index_path: str = "nail_art_index.faiss",
                         metadata_path: str = "nail_art_metadata.pkl") -> None:
    """Build FAISS index from image URLs."""
    import faiss
    
    # Create download directory
    os.makedirs(download_dir, exist_ok=True)
    
    embeddings = []
    valid_metadata = []
    
    print(f"Processing {len(image_urls)} images...")
    
    for i, (url, meta) in enumerate(zip(image_urls, metadata)):
        try:
            # Download image
            image_bytes = download_image(url)
            if image_bytes is None:
                continue
            
            # Save image locally
            filename = f"image_{i}.jpg"
            filepath = os.path.join(download_dir, filename)
            with open(filepath, 'wb') as f:
                f.write(image_bytes)
            
            # Generate embedding using your trained model
            embedding = get_clip_embedding(image_bytes)
            embeddings.append(embedding)
            valid_metadata.append(meta)
            
            if (i + 1) % 10 == 0:
                print(f"Processed {i + 1}/{len(image_urls)} images")
                
        except Exception as e:
            print(f"Failed to process {url}: {str(e)}")
            continue
    
    if not embeddings:
        raise Exception("No valid embeddings generated")
    
    # Convert to numpy array
    embeddings_array = np.array(embeddings, dtype=np.float32)
    
    # Build FAISS index
    dimension = embeddings_array.shape[1]
    index = faiss.IndexFlatIP(dimension)
    
    # Add vectors to index
    index.add(embeddings_array)
    
    # Save index and metadata
    faiss.write_index(index, index_path)
    
    with open(metadata_path, 'wb') as f:
        pickle.dump(valid_metadata, f)
    
    print(f"Built index with {len(embeddings)} vectors")
    print(f"Index saved to {index_path}")
    print(f"Metadata saved to {metadata_path}")
