# -*- coding: utf-8 -*-
"""Nail-Training-Camp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t849mRxlX6VV7VnQlVAIwuPSBH5POECG
"""

!pip install torch torchvision transformers pillow numpy tqdm matplotlib
print("âœ… Dependencies installed!")

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm
import matplotlib.pyplot as plt

# Check GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Using device: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ï¿½ï¿½ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("âš ï¸  No GPU detected - training will be very slow!")
    print("ï¿½ï¿½ Consider using Colab's GPU runtime")

from google.colab import files
import zipfile

print("ğŸ“ Upload your nail art dataset ZIP file (nail_art_colab_dataset.zip):")
uploaded = files.upload()

# Extract the uploaded file
for filename in uploaded.keys():
    if filename.endswith('.zip'):
        print(f"ğŸ“¦ Extracting {filename}...")
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall('nail_art_data')
        print(f"âœ… Extracted successfully!")
        break

# List extracted contents
print("\nğŸ“‹ Extracted files:")
for root, dirs, files in os.walk('nail_art_data'):
    for file in files:
        print(f"  ï¿½ï¿½ {os.path.join(root, file)}")

# Load metadata
metadata_path = 'nail_art_data/metadata.json'
if os.path.exists(metadata_path):
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    print(f"\nğŸ“Š Dataset info:")
    print(f"  ğŸ–¼ï¸  Images: {len(metadata)}")
    for item in metadata:
        print(f"    â€¢ {item['filename']}: {item['description']}")

def create_training_dataset(data_dir):
    """Create training dataset from extracted nail art data."""
    dataset = []

    # Load metadata if available
    metadata_path = os.path.join(data_dir, 'metadata.json')
    if os.path.exists(metadata_path):
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)

        for item in metadata:
            image_path = os.path.join(data_dir, item['filename'])
            if os.path.exists(image_path):
                dataset.append({
                    'image_path': image_path,
                    'description': item['description']
                })
    else:
        # Fallback: scan directory for images
        for filename in os.listdir(data_dir):
            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                dataset.append({
                    'image_path': os.path.join(data_dir, filename),
                    'description': f"Professional nail art design - {filename}"
                })

    return dataset

# Create dataset
dataset = create_training_dataset('nail_art_data')
print(f"âœ… Created dataset with {len(dataset)} images")
print(f"\nï¿½ï¿½ Sample data:")
for i, item in enumerate(dataset[:3]):
    print(f"  {i+1}. {os.path.basename(item['image_path'])}: {item['description']}")

class NailArtDataset(Dataset):
    def __init__(self, data, processor, max_length=77):
        self.data = data
        self.processor = processor
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # Load and process image
        image = Image.open(item['image_path']).convert('RGB')

        # CRITICAL FIX: Process text and image separately with consistent padding
        text_inputs = self.processor(
            text=item['description'],
            return_tensors="pt",
            padding="max_length",  # Force consistent padding
            max_length=self.max_length,
            truncation=True
        )

        image_inputs = self.processor(
            images=image,
            return_tensors="pt",
            padding="max_length"  # Force consistent padding
        )

        # Return with consistent tensor shapes
        return {
            'pixel_values': image_inputs['pixel_values'].squeeze(0),  # Remove batch dim
            'input_ids': text_inputs['input_ids'].squeeze(0),         # Remove batch dim
            'attention_mask': text_inputs['attention_mask'].squeeze(0), # Remove batch dim
            'description': item['description']
        }

print("ï¿½ï¿½ Loading CLIP model...")
model_name = "openai/clip-vit-large-patch14"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# Move to device
model = model.to(device)
print(f"âœ… CLIP model loaded on {device}")
print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"ï¿½ï¿½ Model size: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9:.1f} GB")

print("ğŸ“Š Preparing data loaders...")

# Load CLIP model and processor
print("ï¿½ï¿½ Loading CLIP model...")
model_name = "openai/clip-vit-large-patch14"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# Move to device
model = model.to(device)
print(f"âœ… CLIP model loaded on {device}")
print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"ï¿½ï¿½ Model size: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9:.1f} GB")

# For small datasets, use all data for training (no validation split)
if len(dataset) < 10:
    print(f"ğŸ“ Small dataset detected ({len(dataset)} images) - using all data for training")
    train_dataset = NailArtDataset(dataset, processor)
    val_dataset = None

    # Create data loaders
    batch_size = min(4, len(dataset))  # Small batch size for small datasets
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = None

    print(f"âœ… Data loaders ready")
    print(f"  ğŸš‚ Training: {len(dataset)} images")
    print(f"  ï¿½ï¿½ Batch size: {batch_size}")
    print(f"  âš ï¸  No validation split (dataset too small)")
else:
    # Split dataset (80% train, 20% validation)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])

    # Create datasets
    train_dataset = NailArtDataset(train_data, processor)
    val_dataset = NailArtDataset(val_data, processor)

    # Create data loaders
    batch_size = min(8, len(dataset) // 2)  # Adaptive batch size
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    print(f"âœ… Data loaders ready")
    print(f"  ğŸš‚ Training: {len(train_dataset)} images")
    print(f"  âœ… Validation: {len(val_dataset)} images")
    print(f"  ï¿½ï¿½ Batch size: {batch_size}")

# Training configuration (optimized for small datasets)
learning_rate = 5e-6  # Lower learning rate for small datasets
num_epochs = 5 if len(dataset) < 10 else 10  # Fewer epochs for small datasets
warmup_steps = min(50, len(dataset) * 2)  # Adaptive warmup

# Setup optimizer and scheduler
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Training loop
print(f"ğŸ¯ Starting training for {num_epochs} epochs...")
print(f"ğŸ“š Learning rate: {learning_rate}")
print(f"ğŸ”¥ Warmup steps: {warmup_steps}")
print(f"ï¿½ï¿½ Dataset size: {len(dataset)} images")

best_loss = float('inf')
train_losses = []

for epoch in range(num_epochs):
    print(f"\nğŸ“š Epoch {epoch+1}/{num_epochs}")

    # Training
    model.train()
    train_loss = 0.0

    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Training Epoch {epoch+1}")):
        # Move batch to device - CORRECTED VERSION
        pixel_values = batch['pixel_values'].to(device)
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        # Forward pass with corrected inputs
        outputs = model(
            pixel_values=pixel_values,
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Calculate contrastive loss
        logits_per_image = outputs.logits_per_image
        logits_per_text = outputs.logits_per_text

        batch_size = logits_per_image.size(0)
        labels = torch.arange(batch_size).to(device)

        # Image-to-text and text-to-image loss
        loss_i2t = nn.CrossEntropyLoss()(logits_per_image, labels)
        loss_t2i = nn.CrossEntropyLoss()(logits_per_text, labels)
        loss = (loss_i2t + loss_t2i) / 2

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        # Learning rate warmup
        if batch_idx < warmup_steps:
            lr_scale = min(1., float(batch_idx + 1) / warmup_steps)
            for pg in optimizer.param_groups:
                pg['lr'] = learning_rate * lr_scale

    # Update scheduler
    scheduler.step()

    # Calculate average loss
    avg_train_loss = train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    print(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}")

    # Save best model
    if avg_train_loss < best_loss:
        best_loss = avg_train_loss
        print(f"ğŸ’¾ New best model! Loss: {avg_train_loss:.4f}")

        # Save model
        os.makedirs('fine_tuned_clip', exist_ok=True)
        model.save_pretrained('fine_tuned_clip')
        processor.save_pretrained('fine_tuned_clip')

        # Save training history
        history = {
            'train_losses': train_losses,
            'best_loss': best_loss,
            'epochs': list(range(1, epoch + 2)),
            'dataset_size': len(dataset),
            'learning_rate': learning_rate,
            'num_epochs': num_epochs
        }

        with open('fine_tuned_clip/training_history.json', 'w') as f:
            json.dump(history, f, indent=2)

print(f"\nï¿½ï¿½ Training completed!")
print(f"ğŸ† Best loss: {best_loss:.4f}")
print(f"ğŸ“Š Final training loss: {train_losses[-1]:.4f}")

# Plot training progress
if len(train_losses) > 1:
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, 'b-', label='Training Loss')
    plt.title('CLIP Training Progress')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()